---
title: "Impact of transmission type on car consumption"
author: "Stefano Galeano"
date: "1 July 2017"
output:
  pdf_document:
        number_sections: true
        fig_width: 4
        fig_height: 3
        fig_caption: true
  html_document: default
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In the report we're going to explain how to perform a simple regression analysis using the R built-in dataset  `mtcars`. In Particular we want to answer the following questions: 

> + Is an automatic or manual transmission better for MPG?
> + Quantify the MPG difference between automatic and manual transmissions

The document is structured as follow: after having a look in the dataset, we're going to perform some basic exploratory analysis by computing the correlation matrix and showing a violing plot. In the following section we are going to perform some regression analysis, firstly using a one-variable model (`am`) and secondly adding other predictors in the model. We'll choose the varibles to include in the prevous step by using two different model-selection criteria like *VIF* and *Nested ANOVA* test. At the end we'll check the quality of the model using residual and his derivatives.

<!-- Which are related to the null and alternative hypotesis as follow: -->

<!-- + **null hypotesis**: there is no impact in consumption respect using an automatic or manual transmission (diffrence in mean equal to 0) -->
<!-- + **alternative hypotesis** the transmission type is related to the consumption (difference in mean not equal to 0) -->

# Data Overview

The dataset is composed of eleven variables for each observation an the most interesting ones for our questions are:

* *mpg*: fuel consumption [Miles/(US) gallon] **outcome**
* *cyl*: number of cylinders (4,6 and 8)
* *disp*: Displacement (cu.in.)
* *hp*: wross horsepower
<!-- * *drat*: Rear axle ratio -->
* *wt* weight [1000 lbs]
<!-- * *qsec*: 1/4 mile time -->
* *vs*:  engine type (0 = V, 1 = straight)
* *am*: transmission type (0 = automatic, 1 = manual), **predictor**
<!-- * *gear*: number of forward gears -->
* *carb*: number of carburetors

and the first few rows of the dataset are shown below:

```{r}
nonFactorVars <- c("mpg","cyl","disp","hp","wt","vs","carb")
factorVars <- c("am")
# save into a local variable the dataframe
Mtcars.df <- mtcars[,c(nonFactorVars,factorVars)]
# coerce the categorical variables rappresented as numerical into R factor variables
Mtcars.df[factorVars] <- lapply(Mtcars.df[factorVars], factor)

head(Mtcars.df,5)
```
where we decided to treat `cyl` and `carb` as non categorical variable since that their values has an intrinsec order and the distance between defferent values as well: for instance 4 cylinders are less then 6 and the difference between 4 and 6 cylinders is in some sense less then the difference between 4 and 8 cylinders.

# Exploratory data analysis

Initially, we want to analize the distribution of the car consumption `mpg` for the two group of `am`. We are using a violing plot because is a combination of the bocplot and an histogram and give us more information in one fell swoop:

```{r}
library(ggplot2)

g1 <- ggplot(Mtcars.df,aes(x = am, y = mpg, fill = am))
g1 <- g1 + geom_violin(color = "black")
g1 <- g1 + scale_x_discrete(breaks=c("0","1"),
                            labels=c("Automatic", "Manual"))
g1 <- g1 + geom_point()
g1 <- g1 + xlab("Transmission type")
g1 <- g1 + ylab("Consummption [Miles / US Gallon]")
g1
```

and we can obsereve that it seems to be a clear distinction for the two groups: the mean and the variance are higher for the the manual group respect to the automatic one.  

Secondly, we'are going to calculate the correlation between the non-categorical variables in order to understand which variables make sense to include in the multivariable model.

```{r, fig.height=4}
library(corrplot)

M <- cor(Mtcars.df[nonFactorVars])
corrplot(M, method="circle")
```

where more the dots are `blue`/`red` more the two variable are positevely/negatevely correlated. We've used the `corplot` instead of the `cor` function since that the use of the colors give a better and immediate understanding of the correlation between couple of variables. 

We're using the output of `corplot` in order to choose which variable may be added into the model. We'll see in the next section two criteria that can bu used in order to avoid the well-knonw problem of variance inflation i.e. the conseguences of including predicors higly corellated each other.

From the correlation plot we can note that:

+ `wt`, `disp`, `cyl` and `hp` are highly with `mpg`, which leads us to candidate them as possible predicor variables;
+ `disp` is highly correlated `cyl`, in fact exists a matematical relation between them, and we want to spear to include both in the model.

### Statistical inference

The main goal of statiscal inference is to draw conclusion from a set of data affected by errors. In the next two section we're going to use linear models in order to answer our questions.
The main feature of linear model is to be parsimonious, and for this reason, we are inerested in including as less variables as possible.  

## One-variable linear regression

Before construct a multivariable linear regression model, it's a good practice to start with a simple linear regression model and here below is shown the result of the `summary function`: 

```{r, echo=FALSE}
Mtcars.fitSimple <- lm(mpg ~ am, Mtcars.df)
```

```{r}
summary(Mtcars.fitSimple)
```

whose values are interpreted as follow:

+ **Residuals**: they should be normal distributed with mean `0` and at first sight they seems to be symmetric respect `0` anyway we'll plot them later
+ **(Intercept)**: `17.147` and `1.125` are the *estimate* and the *standard error* for the reference level (automatic group)
+ **am1**: `7.245` and `1.764` are the *estimate* and the *stardard error* for the increase in mean for the manual group respect to the automatic group (reference level); `0.000285` is the *p-value* and since it is much more less then `0.05` we can tate that it is statistically significant i.e. this means that the observed difference in mean `7.245` it was by pure chance is very low. 
+ **Residual standart error**: `4,902` is the standard error from the fitted line
+ **R squared**: `0.3598` is the percentage of variation explained by the model; the *Adjusted R Squared* takes into account the number of predictor used since the R squared it is a quantity that is going to increase even inserting useless predictor. 

# Multivariate linear regression

The solution provided from the simple linear regression model it's sufficient in many scenarios and could allow us to answwer the first question. Of course this is true only if we are not ommtting any important variable, in which case the solution can be drammatically changed leading to wrong conclusion. 

However we're interested in answering also the second question, i.e. we want to quantify our statement and to be as accurate as possible.

## Model selection

Using the candidate variables from the *Exploratory Data Analysis* section we're going to create different model ( $predictors(M_i) \subset predictors(M_{i+1}),  \forall i \in \{1,2,3,4\}$ ). Using criteria like *VIF* and *Nested ANOVA test* we want to discover which variables have a good impact and a low inflation in the solution.

```{r, echo=FALSE}
fit1 <- lm(mpg ~ am, Mtcars.df)
fit2 <- lm(mpg ~ am + disp, Mtcars.df)
fit3 <- lm(mpg ~ am + disp + cyl, Mtcars.df)
fit4 <- lm(mpg ~ am + disp + cyl + hp, Mtcars.df)
fit5 <- lm(mpg ~ am + disp + cyl + hp + wt, Mtcars.df)
```

### Variance Inflation Factor

The *VIF* with refernce to a predictor $x_i$ measures the ratio between the model $M_1$ and the model $M_2$. $M_1$ is the model with all predictors, while $M_2$ is the model constructed using $x_i$ and made all the other predictors uncorrelated. Here below is shown the VIF resulting:

```{r}
library(car)

list("lm(mpg ~ am + disp, Mtcars.df)" = vif(fit2),"lm(mpg ~ am + disp + cyl, Mtcars.df)" = vif(fit3),"lm(mpg ~ am + disp + cyl + hp, Mtcars.df)" = vif(fit4), "lm(mpg ~ am + disp + cyl + hp + wt, Mtcars.df)"= vif(fit5))
```

We can see how `disp` increases significantly from model to model.

The **Nested ANOVA Test** involves the use of nested model like the ones that we created for the computation of the VIF:

 
```{r}
anova(fit1, fit2, fit3, fit4, fit5)
```



For these reason and for the consideration made in the section *Exploratory data analysis*, we are going to add `am`,`wt`,`cyl`, and `hp` in the model:

```{r}
anova(fit1, fit2, fit3, fit4)

Mtcars.fitAll <- lm(mpg ~ am * wt + cyl, Mtcars.df)
summary(Mtcars.fitAll)

plot(Mtcars.fitAll)
```

+ decrease in residual stand error
+ increase in R-squared

+ interpreation cyl
```{r}
library(ggplot2)
g1 <- ggplot(Mtcars.df,aes(Mtcars.fitAll$fitted.values,Mtcars.fitAll$residuals))
g1 <- g1 + geom_point()
g1 <- g1 + xlab("AM fitted")
g1

#centering variable
Mtcars.df$cyl.c <- Mtcars.df$cyl - median(Mtcars.df$cyl)
fit2 <- lm(mpg ~ am + cyl.c, Mtcars.df)
summary(fit2)

g2 <- ggplot(Mtcars.df,aes(fit2$fitted.values,fit2$residuals))
g2 <- g2 + geom_point()
g2 <- g2 + xlab("AM and CYL fitted")
g2

Mtcars.df$disp.c <- Mtcars.df$disp - mean(Mtcars.df$disp)
fit3 <- lm(mpg ~ am + disp.c, Mtcars.df)
summary(fit3)

g3 <- ggplot(Mtcars.df,aes(fit3$fitted.values,fit3$residuals))
g3 <- g3 + geom_point()
g3 <- g3 + xlab("AM and DISP fitted")
g3

pt(q = coef(summary(Mtcars.fitAll))[1,3],df = 29,lower.tail = FALSE)
```


```{r fig.width=3}
str(Mtcars.df)

library(GGally) 

g = ggpairs(Mtcars.df, lower = list(continuous = "smooth", discrete = "facetbar", combo = "dot_no_facet"),columns = c(nonFactorVars, factorVars), showStrips = TRUE, axisLabels = "show")
g

g <- ggplot(Mtcars.df,aes(x = disp, y = mpg, color = am))
g <- g + geom_point()
g
```


```{r}
fit <- lm(mpg ~ am + hp + wt + cyl, mtcars)

summary(fit)

```



```{r}
fit <- lm(mpg ~ am + hp + wt, mtcars)
fitCyl <- lm(mpg ~ am + hp + wt + cyl, mtcars)
fitDisp <- lm(mpg ~ am + hp + wt + disp, mtcars)

anova(fit, fitCyl)
anova(fit, fitDisp)

```

# Conclusione







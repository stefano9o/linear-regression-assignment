---
title: "Impact of transmission type on car consumption"
author: "Stefano Galeano"
date: "1 July 2017"
output:
  pdf_document:
        number_sections: true
        fig_width: 4
        fig_height: 3
        fig_caption: true
  html_document: default
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

We're going to perform a simple regression analysis using `mtcars`, an R built-in dataset. In Particular, we are interested in the following questions:

> + Is an automatic or manual transmission better for petrol consumption?
> + Quantify the consumption difference between automatic and manual transmissions

The document is structured as follow: after having a look into the dataset, we're going to perform some basic exploratory analysis by computing the correlation matrix and showing a violin plot. After that we are going to perform some regression analysis, firstly, using a simple regression model (`am`) and secondly, adding other predictors to the model. We'll choose the variables to include in the multivariable model by using two different model-selection criteria: *VIF* and *Nested ANOVA* test. At the end, we'll check the quality of the model using residual and diagnostic values like `hatvalues` and `dfbetas`.

<!-- Which is related to the null and alternative hypothesis as follow: -->

<!-- + **null hypotesis**: there is no impact in consumption respect using an automatic or manual transmission (diffrence in mean equal to 0) -->
<!-- + **alternative hypotesis** the transmission type is related to the consumption (difference in mean not equal to 0) -->

# Data Overview

The dataset is composed of 32 observations with eleven variables for each observation.

In addition to the variables `mpg` (consumption) and `am` (transmission type), the most interesting for our purpose are: displscement [cu.in.] `disp`, horsepower `hp`, number of cylinders `cyl` and weight `wt` [1000 lbs].

<!-- * *mpg*: fuel consumption [Miles/(US) gallon] **outcome** -->
<!-- * *cyl*: number of cylinders (4,6 and 8) -->
<!-- * *disp*: Displacement (cu.in.) -->
<!-- * *hp*: wross horsepower -->
<!-- * *drat*: Rear axle ratio -->
<!-- * *wt* weight [1000 lbs] -->
<!-- * *qsec*: 1/4 mile time -->
<!-- * *vs*:  engine type (0 = V, 1 = straight) -->
<!-- * *am*: transmission type (0 = automatic, 1 = manual), **predictor** -->
<!-- * *gear*: number of forward gears -->
<!-- * *carb*: number of carburetors -->

```{r, echo=FALSE}
#subset of interest
nonFactorVars <- c("mpg","cyl","disp","hp","wt")
factorVars <- c("am")
# save the dataframe into a local variable
Mtcars.df <- mtcars[,c(nonFactorVars,factorVars)]
# coerce the categorical variables rappresented as numerical into R factor variables
Mtcars.df[factorVars] <- lapply(Mtcars.df[factorVars], factor)
```

The first few rows of the dataset are shown below:

```{r}
head(Mtcars.df)
```

where `am` 0 means automatic and 1 means manual transmission. We want to point out that we decided to treat `cyl` and `carb` as non-categorical variable since their values have an intrinsic order and the distance between different is important (for instance 4 cylinders are less than 6 and the difference between 4 and 6 cylinders is less than the difference between 4 and 8 cylinders).

# Exploratory data analysis

Initially, we want to analyse the distribution of the car consumption `mpg` for the two group manual/automatic:

```{r echo=FALSE}
library(ggplot2)

g <- ggplot(Mtcars.df,aes(x = am, y = mpg, fill = am))
g <- g + geom_violin(color = "black")
g <- g + scale_x_discrete(breaks=c("0","1"),
                            labels=c("Automatic", "Manual"))
g <- g + geom_point()
g <- g + xlab("Transmission type")
g <- g + ylab("Consummption [Miles / US Gallon]")
g
```

and we can observe that it seems to be a clear distinction between the two groups.

We're now going to calculate the correlation between the non-categorical variables in order to understand which variables make sense to include in the multivariable model:

```{r,  echo=FALSE, fig.height=4}
library(corrplot)

M <- cor(Mtcars.df[nonFactorVars])
corrplot(M, method="circle")
```

More the dots are `blue` or `red` more the two variables are positively/negatively correlated.

We'll see in the next section two criteria that can bu used in order to avoid the well-known problem of variance inflation i.e. the consequences of including predictors highly correlated each other.

From the correlation plot, we can note that:

+ `wt`, `disp`, `cyl` and `hp` are highly correlated with `mpg`, which leads us to candidate them as possible predictor variables;
+ `disp` is highly correlated `cyl`, in fact, exists a mathematical relation between them, and we want to spear to include both in the model.

## Model selection

Using the variables from the previous section we're going to create different model ( $predictors(M_i) \subset predictors(M_{i+1}),  \forall i \in \{1,2,3,4\}$ ). Using criteria like *VIF* and *Nested ANOVA test* we want to discover which variables have a good impact and a low inflation in the solution:

```{r}
fit1 <- lm(mpg ~ am, Mtcars.df)
fit2 <- lm(mpg ~ am + wt, Mtcars.df)
fit3 <- lm(mpg ~ am + wt + cyl, Mtcars.df)
fit4 <- lm(mpg ~ am + wt + cyl + hp, Mtcars.df)
fit5 <- lm(mpg ~ am + wt + cyl + hp + disp, Mtcars.df)
```

### Variance Inflation Factor

The *VIF* quantify the severity of multicollinearity for each predictor. Calculating The VIF for each model defined previously, we can see the trend of each coefficient as we add new predictors into the model:

```{r, echo=FALSE}
library(car)
```

| Model/VIF                |          am          |          wt          |         cyl          |          hp          |          disp        |
|--------------------------|:--------------------:|:--------------------:|:--------------------:|:--------------------:|:--------------------:|
|          `fit1`          |        ```1```       |                      |                      |                      |                      |
|   `fit2` (added `wt`)    | ```r vif(fit2)[1]``` | ```r vif(fit2)[2]``` |                      |                      |                      |
|   `fit3` (added `cyl`)   | ```r vif(fit3)[1]``` | ```r vif(fit3)[2]``` | ```r vif(fit3)[3]``` |                      |                      |
|   `fit4` (added `hp`)    | ```r vif(fit4)[1]``` | ```r vif(fit4)[2]``` | ```r vif(fit4)[3]``` | ```r vif(fit4)[4]``` |                      |
|   `fit5` (added `disp`)  | ```r vif(fit5)[1]``` | ```r vif(fit5)[2]``` | ```r vif(fit5)[3]``` | ```r vif(fit5)[4]``` | ```r vif(fit5)[5]``` |


From the previous table, we can notice that all the VIFs increase considerably after inserting `hp`.

### Nested ANOVA Test
The **Nested ANOVA Test** involves the use of nested models like the ones that we created for the computation of the VIF:

 
```{r, echo=FALSE}
Mtcars.anova <- anova(fit1, fit2, fit3, fit4, fit5)
Mtcars.anova
```

and even here we notice that inserting `hp` and `disp` doesn't have a significant impact on the model.

# Statistical inference

The main goal of statistical inference is to draw conclusions from a set of data affected by errors. In the next two section, we're going to use linear models in order to answer our questions.

## Simple linear regression

Before constructing a multivariable linear regression model, it's a good practice to start with a simple linear regression model and here below will be shown one at a time the output of the `summary function`.

```{r, echo=FALSE}
Mtcars.fitSimple <- lm(mpg ~ am, Mtcars.df)
```

```{r}
summary(resid(Mtcars.fitSimple))
```

The residuals should be normally distributed with mean `0` and, at first sight, they seem to be quite uncorrelated and symmetric respect `0`.

```{r}
summary(Mtcars.fitSimple)$coefficients
```

The coefficients `17.147` and `1.125` are the *estimate* and the *standard error* for the reference level (automatic group) while the coefficients `7.245` and `1.764` are the *estimate* and the *standard error* for the increase in mean for the manual group respect to the automatic group (reference level).

At this stage, since the *p-value* `0.000285` is much less than `0.05`, the difference in mean is statistically significant.

```{r}
summary(Mtcars.fitSimple)$sigma
```

The Residual standard error is the standard error of the observed data from the fitted line

```{r}
summary(Mtcars.fitSimple)$r.squared

summary(Mtcars.fitSimple)$adj.r.squared
```

The `R-squared` `0.360` is the percentage of variation explained by the model and the `Adjusted R-squared` `0.338` mesaure the same quantity but takes also into account the number of predictors used.

## Multivariable linear regression
The main feature of the linear model is to be parsimonious, and for this reason, we are interested in including as fewer variables as possible. Using the results made with the model selection methods, we are going to add `wt` and `cyl` into the multivariable regression model:

```{r}
Mtcars.fitMulti <- lm(mpg ~ am + wt + cyl, Mtcars.df)

summary(Mtcars.fitMulti)$coef
```

where we can observe how after inserting `wt` and `cyl` into the model, changes the results for `am` because now the difference between automatic and manual transmission is very little and the `p-value` `0.89334` is no longer statistically significant.

# Diagnostic and residual analysis

The most important approach for understanding poor model fitting is the analysis of the residuals, which are the vertical distance of the observed data from the fitted line. In fact, they have the ability to zoom potential problems. From the theory, we know that the residuals should be uncorrelated the one from the other with mean 0, and for this reason, the plot shouldn't show any pattern. Here below it's shown the residual vs the fitted value:

```{r echo=FALSE}
library(ggplot2)

g <- ggplot(data.frame(x = fitted(Mtcars.fitMulti),y = resid(Mtcars.fitMulti)),aes(x = x, y = y))
g <- g + geom_point()
g <- g + geom_hline(yintercept = 0)
g <- g + xlab("Fitted Values")
g <- g + ylab("Residuals")
g
```

As we can see from the plot, and from the output of the `fit` function in section 3.1, the residuals seem to be quite uncorrelated and distributed around 0.

`dfbetas` and `hatvalues` are two examples of diagnostic value and they are able to find out different problems. Like the residuals, they are calculated for each observed data, and they measure the **influence** and the **laverage**. The first indicates how much the solution changes without including the point itself. while the second indicates how much the point is far from the cloud of data. We're going now to print the output of the `hatvalues` function: 

```{r}
Mtcars.hatvalues <- hatvalues(Mtcars.fitMulti)

summary(Mtcars.hatvalues)
head(Mtcars.hatvalues[order(Mtcars.hatvalues)])
```

# Conclusion

